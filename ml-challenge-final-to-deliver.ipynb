{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Programming Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:07.394433Z",
     "iopub.status.busy": "2025-10-13T12:42:07.393518Z",
     "iopub.status.idle": "2025-10-13T12:42:07.404072Z",
     "shell.execute_reply": "2025-10-13T12:42:07.403064Z",
     "shell.execute_reply.started": "2025-10-13T12:42:07.394393Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ml-challenge-datasets/EvaluateOnMe.csv\n",
      "/kaggle/input/ml-challenge-datasets/TrainOnMe_orig.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and boring stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:07.405924Z",
     "iopub.status.busy": "2025-10-13T12:42:07.405582Z",
     "iopub.status.idle": "2025-10-13T12:42:07.428032Z",
     "shell.execute_reply": "2025-10-13T12:42:07.426957Z",
     "shell.execute_reply.started": "2025-10-13T12:42:07.405890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: /kaggle/input/ml-challenge-datasets/TrainOnMe_orig.csv\n",
      "Evaluation data: /kaggle/input/ml-challenge-datasets/EvaluateOnMe.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "base_path = '/kaggle/input/ml-challenge-datasets/'\n",
    "output_path = '/kaggle/working/'\n",
    "dataset_ev = base_path + 'EvaluateOnMe.csv'\n",
    "dataset_tr = base_path + 'TrainOnMe_orig.csv'\n",
    "\n",
    "print('Train data: ' + dataset_tr + '\\nEvaluation data: ' + dataset_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:07.429400Z",
     "iopub.status.busy": "2025-10-13T12:42:07.429041Z",
     "iopub.status.idle": "2025-10-13T12:42:07.489736Z",
     "shell.execute_reply": "2025-10-13T12:42:07.488777Z",
     "shell.execute_reply.started": "2025-10-13T12:42:07.429368Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Dataset loaded, Lesssgoooooooo!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(dataset_tr, encoding=\"utf-8\")\n",
    "eval_df = pd.read_csv(dataset_ev)\n",
    "print(\"Dataset loaded, Lesssgoooooooo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:07.491131Z",
     "iopub.status.busy": "2025-10-13T12:42:07.490795Z",
     "iopub.status.idle": "2025-10-13T12:42:07.504953Z",
     "shell.execute_reply": "2025-10-13T12:42:07.503837Z",
     "shell.execute_reply.started": "2025-10-13T12:42:07.491101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUICK DATASET INSPECTION (just for fun)\n",
      "\n",
      "Training set shape: (1000, 14)\n",
      "Evaluation set shape: (10000, 13)\n",
      "\n",
      "Label distribution:\n",
      "y\n",
      "Andjorg     409\n",
      "Andsuto     334\n",
      "Jorgsuto    257\n",
      "Name: count, dtype: int64\n",
      "Number of unique labels: 3\n",
      "Missing label cells (NaN or blank): 0 0\n",
      "\n",
      "Label relative frequencies (proportions):\n",
      "y\n",
      "Andjorg     0.409\n",
      "Andsuto     0.334\n",
      "Jorgsuto    0.257\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Minimum examples in any class: 257\n"
     ]
    }
   ],
   "source": [
    "label_col = \"y\"\n",
    "\n",
    "# basic label cleaning (keep exact characters but remove stray whitespace)\n",
    "train_df[label_col] = train_df[label_col].astype(str).str.strip()\n",
    "\n",
    "# quick checks on labels\n",
    "print(\"QUICK DATASET INSPECTION (just for fun)\\n\")\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Evaluation set shape: {eval_df.shape}\")\n",
    "print(f\"\\nLabel distribution:\\n{train_df[label_col].value_counts()}\")\n",
    "\n",
    "print(\"Number of unique labels:\", train_df[label_col].nunique())\n",
    "print(\"Missing label cells (NaN or blank):\", train_df[label_col].isna().sum(), train_df[label_col].eq(\"\").sum())\n",
    "print(\"\\nLabel relative frequencies (proportions):\")\n",
    "print(train_df[label_col].value_counts(normalize=True))\n",
    "print(\"\\nMinimum examples in any class:\", train_df[label_col].value_counts().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:07.508280Z",
     "iopub.status.busy": "2025-10-13T12:42:07.507999Z",
     "iopub.status.idle": "2025-10-13T12:42:07.534586Z",
     "shell.execute_reply": "2025-10-13T12:42:07.533560Z",
     "shell.execute_reply.started": "2025-10-13T12:42:07.508229Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original labels: ['Jorgsuto' 'Andjorg' 'Andsuto']\n",
      "  Dropping constant column 'x12' (value: True)\n",
      "\n",
      "Features after cleaning: 12\n",
      "Feature names: ['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x13']\n",
      "Label encoding:\n",
      "  Andjorg -> 0\n",
      "  Andsuto -> 1\n",
      "  Jorgsuto -> 2\n",
      "\n",
      "Categorical features (1): ['x7']\n",
      "Numeric features (11): 11 features\n"
     ]
    }
   ],
   "source": [
    "# Store exact labels\n",
    "original_labels = train_df[label_col].unique()\n",
    "print(f\"Original labels: {original_labels}\")\n",
    "\n",
    "# Check for constant columns\n",
    "constant_cols = []\n",
    "for col in train_df.columns:\n",
    "    if col != label_col and train_df[col].nunique() == 1:\n",
    "        constant_cols.append(col)\n",
    "        print(f\"  Dropping constant column '{col}' (value: {train_df[col].iloc[0]})\")\n",
    "\n",
    "if not constant_cols:\n",
    "    print(\"  No constant columns found\")\n",
    "\n",
    "# Separate features and target\n",
    "X = train_df.drop(columns=[label_col] + constant_cols)\n",
    "y = train_df[label_col]\n",
    "X_eval = eval_df.drop(columns=constant_cols) if constant_cols else eval_df.copy()\n",
    "\n",
    "print(f\"\\nFeatures after cleaning: {X.shape[1]}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Label encoding:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label} -> {idx}\")\n",
    "\n",
    "# Identify feature types\n",
    "categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"Numeric features ({len(numeric_features)}): {len(numeric_features)} features\")\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Random Forest Evaluation\n",
    "Note: I tried different models, but I will report here the best one I found, that is the Random Forest.\n",
    "I tried decision trees and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:07.536199Z",
     "iopub.status.busy": "2025-10-13T12:42:07.535779Z",
     "iopub.status.idle": "2025-10-13T12:42:09.203851Z",
     "shell.execute_reply": "2025-10-13T12:42:09.202993Z",
     "shell.execute_reply.started": "2025-10-13T12:42:07.536177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest (n_estimators=200, default params):\n",
      "  CV scores: [0.835 0.845 0.85  0.845 0.82 ]\n",
      "  Mean accuracy: 0.8390 (+/- 0.0107)\n",
      "  Range: [0.8200, 0.8500]\n"
     ]
    }
   ],
   "source": [
    "# Create baseline pipeline\n",
    "baseline_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "k_splits = 5\n",
    "cv = StratifiedKFold(n_splits=k_splits, shuffle=True, random_state=42)\n",
    "baseline_scores = cross_val_score(baseline_rf, X, y_encoded, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(f\"Baseline Random Forest (n_estimators=200, default params):\")\n",
    "print(f\"  CV scores: {baseline_scores}\")\n",
    "print(f\"  Mean accuracy: {baseline_scores.mean():.4f} (+/- {baseline_scores.std():.4f})\")\n",
    "print(f\"  Range: [{baseline_scores.min():.4f}, {baseline_scores.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "Let's see if he does a better job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:42:09.205210Z",
     "iopub.status.busy": "2025-10-13T12:42:09.204886Z",
     "iopub.status.idle": "2025-10-13T12:43:33.947434Z",
     "shell.execute_reply": "2025-10-13T12:43:33.946315Z",
     "shell.execute_reply.started": "2025-10-13T12:42:09.205189Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter space:\n",
      "  n_estimators: [200, 300, 400, 500]\n",
      "  max_depth: [10, 15, 20, 25, 30, None]\n",
      "  min_samples_split: [2, 5, 10, 15]\n",
      "  min_samples_leaf: [1, 2, 4, 6]\n",
      "  max_features: ['sqrt', 'log2', 0.5, 0.7]\n",
      "  class_weight: [None, 'balanced']\n",
      "\n",
      "Starting RandomizedSearchCV (30 iterations, 5-fold CV)...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Tuning complete!, Lessssgooooooooo\n",
      "\n",
      "Best parameters found:\n",
      "  n_estimators: 400\n",
      "  min_samples_split: 2\n",
      "  min_samples_leaf: 2\n",
      "  max_features: 0.7\n",
      "  max_depth: 15\n",
      "  class_weight: None\n",
      "  bootstrap: True\n",
      "\n",
      "Best CV accuracy: 0.8530\n",
      "Improvement over baseline: +0.0140 (+1.40%)\n"
     ]
    }
   ],
   "source": [
    "# Create tuning pipeline\n",
    "tuning_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [200, 300, 400, 500],\n",
    "    'classifier__max_depth': [10, 15, 20, 25, 30, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10, 15],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4, 6],\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.5, 0.7],\n",
    "    'classifier__bootstrap': [True],\n",
    "    'classifier__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "print(f\"Parameter space:\")\n",
    "print(f\"  n_estimators: {param_grid['classifier__n_estimators']}\")\n",
    "print(f\"  max_depth: {param_grid['classifier__max_depth']}\")\n",
    "print(f\"  min_samples_split: {param_grid['classifier__min_samples_split']}\")\n",
    "print(f\"  min_samples_leaf: {param_grid['classifier__min_samples_leaf']}\")\n",
    "print(f\"  max_features: {param_grid['classifier__max_features']}\")\n",
    "print(f\"  class_weight: {param_grid['classifier__class_weight']}\")\n",
    "\n",
    "# Randomized search\n",
    "n_iteration = 30\n",
    "print(f\"\\nStarting RandomizedSearchCV ({n_iteration} iterations, {k_splits}-fold CV)...\")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    tuning_rf,\n",
    "    param_grid,\n",
    "    n_iter=n_iteration,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X, y_encoded)\n",
    "\n",
    "print(f\"Tuning complete!, Lessssgooooooooo\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param.replace('classifier__', '')}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV accuracy: {random_search.best_score_:.4f}\")\n",
    "print(f\"Improvement over baseline: {random_search.best_score_ - baseline_scores.mean():+.4f} ({(random_search.best_score_ - baseline_scores.mean())*100:+.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:43:33.948757Z",
     "iopub.status.busy": "2025-10-13T12:43:33.948480Z",
     "iopub.status.idle": "2025-10-13T12:43:38.662822Z",
     "shell.execute_reply": "2025-10-13T12:43:38.661728Z",
     "shell.execute_reply.started": "2025-10-13T12:43:33.948735Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cross-validated predictions for error analysis...\n",
      "\n",
      "Confusion Matrix:\n",
      "          Andjorg  Andsuto  Jorgsuto\n",
      "Andjorg       383        3        23\n",
      "Andsuto        11      307        16\n",
      "Jorgsuto       60       34       163\n",
      "\n",
      "(Rows=True labels, Columns=Predicted labels)\n",
      "\n",
      "Per-Class Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Andjorg     0.8436    0.9364    0.8876       409\n",
      "     Andsuto     0.8924    0.9192    0.9056       334\n",
      "    Jorgsuto     0.8069    0.6342    0.7102       257\n",
      "\n",
      "    accuracy                         0.8530      1000\n",
      "   macro avg     0.8477    0.8299    0.8345      1000\n",
      "weighted avg     0.8505    0.8530    0.8480      1000\n",
      "\n",
      "Per-Class Accuracy:\n",
      "  Andjorg: 0.9364 (409 samples)\n",
      "  Andsuto: 0.9192 (334 samples)\n",
      "  Jorgsuto: 0.6342 (257 samples)\n"
     ]
    }
   ],
   "source": [
    "# Get best model\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Get cross-validated predictions\n",
    "print(\"Generating cross-validated predictions for error analysis...\")\n",
    "y_pred_cv = cross_val_predict(best_rf, X, y_encoded, cv=cv, n_jobs=-1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_encoded, y_pred_cv)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=label_encoder.classes_,\n",
    "                     columns=label_encoder.classes_)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_df)\n",
    "print(\"\\n(Rows=True labels, Columns=Predicted labels)\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "report = classification_report(y_encoded, y_pred_cv,\n",
    "                              target_names=label_encoder.classes_,\n",
    "                              digits=4)\n",
    "print(report)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"Per-Class Accuracy:\")\n",
    "for idx, label in enumerate(label_encoder.classes_):\n",
    "    class_mask = y_encoded == idx\n",
    "    class_acc = (y_pred_cv[class_mask] == idx).sum() / class_mask.sum()\n",
    "    class_samples = class_mask.sum()\n",
    "    print(f\"  {label}: {class_acc:.4f} ({class_samples} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the final model on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:43:38.664305Z",
     "iopub.status.busy": "2025-10-13T12:43:38.663919Z",
     "iopub.status.idle": "2025-10-13T12:43:40.331633Z",
     "shell.execute_reply": "2025-10-13T12:43:40.330390Z",
     "shell.execute_reply.started": "2025-10-13T12:43:38.664274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model with best parameters on all training data...\n",
      "Training complete, lesssgoooo again\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "  1. x4: 0.4337\n",
      "  2. x11: 0.2367\n",
      "  3. x10: 0.1197\n",
      "  4. x9: 0.0553\n",
      "  5. x8: 0.0429\n",
      "  6. x6: 0.0255\n",
      "  7. x3: 0.0222\n",
      "  8. x2: 0.0208\n",
      "  9. x13: 0.0136\n",
      "  10. x5: 0.0116\n"
     ]
    }
   ],
   "source": [
    "# Train on entire training set\n",
    "print(\"Training final model with best parameters on all training data...\")\n",
    "best_rf.fit(X, y_encoded)\n",
    "print(\"Training complete, lesssgoooo again\")\n",
    "\n",
    "# Show feature importance (top 10)\n",
    "if hasattr(best_rf.named_steps['classifier'], 'feature_importances_'):\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = numeric_features.copy()\n",
    "    if categorical_features:\n",
    "        cat_encoder = best_rf.named_steps['preprocessor'].named_transformers_['cat']\n",
    "        if hasattr(cat_encoder, 'get_feature_names_out'):\n",
    "            cat_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "            feature_names.extend(cat_names)\n",
    "\n",
    "    importances = best_rf.named_steps['classifier'].feature_importances_\n",
    "\n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for i in range(min(10, len(importances))):\n",
    "        idx = indices[i]\n",
    "        feat_name = feature_names[idx] if idx < len(feature_names) else f\"Feature_{idx}\"\n",
    "        print(f\"  {i+1}. {feat_name}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:43:40.332938Z",
     "iopub.status.busy": "2025-10-13T12:43:40.332607Z",
     "iopub.status.idle": "2025-10-13T12:43:40.578571Z",
     "shell.execute_reply": "2025-10-13T12:43:40.577575Z",
     "shell.execute_reply.started": "2025-10-13T12:43:40.332902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n",
      "Generated 10000 predictions\n",
      "\n",
      "Prediction distribution:\n",
      "  Andjorg: 4361 (43.6%)\n",
      "  Andsuto: 3347 (33.5%)\n",
      "  Jorgsuto: 2292 (22.9%)\n",
      "\n",
      "Training distribution (just for comparison):\n",
      "  Andjorg: 409 (40.9%)\n",
      "  Andsuto: 334 (33.4%)\n",
      "  Jorgsuto: 257 (25.7%)\n"
     ]
    }
   ],
   "source": [
    "# Predict on evaluation data\n",
    "print(\"Generating predictions...\")\n",
    "y_eval_encoded = best_rf.predict(X_eval)\n",
    "\n",
    "# Decode back to original labels\n",
    "y_eval_pred = label_encoder.inverse_transform(y_eval_encoded)\n",
    "\n",
    "print(f\"Generated {len(y_eval_pred)} predictions\")\n",
    "\n",
    "# Show prediction distribution\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "unique, counts = np.unique(y_eval_pred, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    percentage = count / len(y_eval_pred) * 100\n",
    "    print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Compare with training distribution\n",
    "print(f\"\\nTraining distribution (just for comparison):\")\n",
    "train_unique, train_counts = np.unique(y, return_counts=True)\n",
    "for label, count in zip(train_unique, train_counts):\n",
    "    percentage = count / len(y) * 100\n",
    "    print(f\"  {label}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:43:40.580073Z",
     "iopub.status.busy": "2025-10-13T12:43:40.579477Z",
     "iopub.status.idle": "2025-10-13T12:43:40.589639Z",
     "shell.execute_reply": "2025-10-13T12:43:40.588712Z",
     "shell.execute_reply.started": "2025-10-13T12:43:40.580049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved\n"
     ]
    }
   ],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({'y': y_eval_pred})\n",
    "output_file = 'predictions.txt'\n",
    "\n",
    "# Save WITHOUT header and WITHOUT index\n",
    "with open(output_file, 'w') as f:\n",
    "    for label in y_eval_pred:\n",
    "        f.write(f\"{label}\\n\")\n",
    "\n",
    "\n",
    "print(\"Predictions saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double check to verify the format of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T12:43:40.590933Z",
     "iopub.status.busy": "2025-10-13T12:43:40.590445Z",
     "iopub.status.idle": "2025-10-13T12:43:40.611380Z",
     "shell.execute_reply": "2025-10-13T12:43:40.610462Z",
     "shell.execute_reply.started": "2025-10-13T12:43:40.590909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying output format:\n",
      "First 5 lines of output:\n",
      "  Line 1: Andsuto\n",
      "  Line 2: Andjorg\n",
      "  Line 3: Jorgsuto\n",
      "  Line 4: Andsuto\n",
      "  Line 5: Andsuto\n",
      "\n",
      "Format verification:\n",
      "  ✓ No header: ✓ PASS\n",
      "  ✓ Labels match exactly: ✓ PASS\n",
      "  ✓ Correct number of predictions (10000): ✓ PASS\n",
      "\n",
      "✓ All format checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Verify output format. Yes I am afraid of\n",
    "print(\"\\nVerifying output format:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    first_lines = [f.readline().strip() for _ in range(5)]\n",
    "    print(\"First 5 lines of output:\")\n",
    "    for i, line in enumerate(first_lines, 1):\n",
    "        print(f\"  Line {i}: {line}\")\n",
    "\n",
    "# Check format\n",
    "no_header_check = first_lines[0] in original_labels\n",
    "labels_match_check = all(line in original_labels for line in first_lines)\n",
    "line_count = sum(1 for _ in open(output_file))\n",
    "correct_length = line_count == len(y_eval_pred)\n",
    "\n",
    "print(\"\\nFormat verification:\")\n",
    "print(f\"  ✓ No header: {'✓ PASS' if no_header_check else '✗ FAIL - CRITICAL ERROR'}\")\n",
    "print(f\"  ✓ Labels match exactly: {'✓ PASS' if labels_match_check else '✗ FAIL - CRITICAL ERROR'}\")\n",
    "print(f\"  ✓ Correct number of predictions ({len(y_eval_pred)}): {'✓ PASS' if correct_length else '✗ FAIL'}\")\n",
    "\n",
    "if not (no_header_check and labels_match_check and correct_length):\n",
    "    print(\"\\n⚠️  WARNING: Format check failed! Review the output before submission.\")\n",
    "else:\n",
    "    print(\"\\n✓ All format checks passed!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8451844,
     "sourceId": 13330773,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
